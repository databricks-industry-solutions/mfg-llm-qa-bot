# Databricks notebook source
# MAGIC %md You may find this notebook on https://github.com/databricks-industry-solutions/mfg-llm-qa-bot.

# COMMAND ----------

# MAGIC %md ##Define Basic Search
# MAGIC
# MAGIC The core idea of agents is to use a language model to choose a sequence of actions (generated by reasoning steps) to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. Agents use tools to interact with the external world.
# MAGIC
# MAGIC In this notebook, we will use wikipedia to answer our question and use the generated response as additional input along with our Vector search responses to answer our question
# MAGIC
# MAGIC **Sequence of Steps**
# MAGIC * zero-shot-react-description agent with the wikipedia tool to answer our question
# MAGIC * TransformChain to combine the results of our Vector Search and our Wikipedia response
# MAGIC * Call the LLM model with the new context.
# MAGIC * Sequence all above steps with a Sequential chain
# MAGIC
# MAGIC
# MAGIC <p>
# MAGIC     <img src="https://github.com/databricks-industry-solutions/mfg-llm-qa-bot/raw/main/images/Basic-similarity-search.png" width="700" />
# MAGIC </p>
# MAGIC
# MAGIC This notebook was tested on the following infrastructure:
# MAGIC * DBR 13.3ML (GPU)
# MAGIC * g5.2xlarge(AWS) - however comparable infra on Azure should work (A10s)

# COMMAND ----------

# MAGIC %md
# MAGIC CUDA [memory management flag](https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

# COMMAND ----------

# MAGIC %sh export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'

# COMMAND ----------

# MAGIC %md
# MAGIC Install Libraries

# COMMAND ----------

# MAGIC %pip install --upgrade langchain==0.1.6 sqlalchemy==2.0.27 transformers==4.37.2 databricks-vectorsearch==0.22 mlflow[databricks] xformers==0.0.24  accelerate==0.27.0  wikipedia==1.4.0

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %run "./utils/configs"

# COMMAND ----------

from langchain.prompts import PromptTemplate
from databricks.vector_search.client import VectorSearchClient
from langchain.vectorstores import DatabricksVectorSearch
from langchain.embeddings import DatabricksEmbeddings
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.agents import load_tools
from langchain.chains import TransformChain
from langchain.chains import SequentialChain
from langchain.memory import SimpleMemory
from langchain.llms import OpenAI



# COMMAND ----------

# MAGIC %md
# MAGIC Get the Vector store retriever

# COMMAND ----------

def get_retriever():
    '''Get the langchain vector retriever from the Databricks object '''
    vsc = VectorSearchClient(workspace_url=configs["DATABRICKS_URL"], personal_access_token=configs['DATABRICKS_TOKEN'])  
    index = vsc.get_index(endpoint_name=configs['vector_endpoint_name'], 
                          index_name=f"{configs['source_catalog']}.{configs['source_schema']}.{configs['vector_index']}")

    index.describe()
    # Create the langchain retriever. text_columns-> chunks column
    # return columns metadata_name and path along with results.
    # embedding is None for Databricks managed embedding
    vectorstore = DatabricksVectorSearch(
        index, text_column="chunks", embedding=None, columns=['metadata_name', 'path']
    )
    #filter isnt working here
    return vectorstore.as_retriever(search_kwargs={"k": configs["num_similar_docs"]}, search_type = "similarity")


# test our retriever
retriever = get_retriever()
similar_documents = retriever.get_relevant_documents("How can I contact OSHA?")
print(f"Relevant documents: {similar_documents}")

# COMMAND ----------

# MAGIC %md
# MAGIC Initialize Langchain Agent with the wikipedia tool. This will pull up results returned form wikipedia that we will add to our context
# MAGIC
# MAGIC We also have an additional input for metadata. This is not used by this agent but passed into our next chain to filter our vector database results only to the metadata_name specified

# COMMAND ----------

# MAGIC %md 
# MAGIC First create our External model endpoint

# COMMAND ----------

import mlflow.deployments
from mlflow.deployments import get_deploy_client

# MLFLow Deployments
mlflow_deploy_client = mlflow.deployments.get_deploy_client("databricks")

try:
  openaikey = f"{{{{secrets/solution-accelerator-cicd/openai_api}}}}" #change to your key
  print(openaikey)
  print('1')
  mlflow_deploy_client.create_endpoint(
    name=f"{configs['serving_endpoint_name']}_rkm",
    config={
      "served_entities": [{
          "external_model": {
              "name": "gpt-3.5-turbo-instruct",
              "provider": "openai",
              "task": "llm/v1/completions",
              "openai_config": {
                  "openai_api_key": openaikey
              }
          }
      }]
    }
  )
except Exception as e:
  print(e)


# COMMAND ----------

# MAGIC %md Test if our endpoint works

# COMMAND ----------

completions_response = mlflow_deploy_client.predict(
    endpoint=f"{configs['serving_endpoint_name']}_rkm",
    inputs={
        "prompt": "How is ph level calculated",
        "temperature": 0.1,
        "max_tokens": 1000,
        "n": 2
    }
)
print(completions_response)

# COMMAND ----------

# MAGIC %md
# MAGIC Create our LLM model from the external endpoint

# COMMAND ----------

from langchain.llms import Databricks
llmai = Databricks(endpoint_name=f"{configs['serving_endpoint_name']}_rkm", extra_params={"temperature": 0.1, "max_tokens": 1000})

# COMMAND ----------

# MAGIC %md 
# MAGIC Initalize our langchain agent and wikipedia tool
# MAGIC
# MAGIC Use the zero shot react description agent

# COMMAND ----------

tools = load_tools(["wikipedia"], llm=llmai)

agent = initialize_agent(tools,
                         llmai,
                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
                         verbose=False,
                         max_iterations=2,
                         early_stopping_method="generate",
                         agent_kwargs={'prefix':"you're an AI chemist who looks up properties of chemical elements. Return valid responses. Do not respond with unhelpful answers. Limit response to 150 words"}
                         )

#test our agent. 
out = agent({'input':"what are properties of acetone", 'metadata':'ACETONE'})
print(f"{agent.agent.input_keys} {agent.agent.return_values}")
print(out)


# COMMAND ----------

# MAGIC %md
# MAGIC Create a TransformChain that adds up the wiki results with our vector search results. 
# MAGIC
# MAGIC Vector search results are filtered to only those that match our metadata input.

# COMMAND ----------

def retrieval_transform(inputs: dict) -> dict:
    docs = retriever.get_relevant_documents(query=inputs["input"])
    docsc = [d.page_content for d in docs if inputs['metadata'] in d.metadata['metadata_name']]
    combineddocs = "\n---\n".join(docsc) + "\n--\n" + inputs['output']
    docs_dict = {
        "question": inputs["input"],
        "context":  combineddocs
    }
    return docs_dict

retrieval_chain = TransformChain(
    input_variables=["input", "output", "metadata"], #output from wiki chain
    output_variables=["question", "context"],
    transform=retrieval_transform
)

print(f"{retrieval_chain.input_keys}-{retrieval_chain.output_keys}")

#test our retrieval chain
out = retrieval_chain({'input':'whats the color of acetone?', 'output':'acetone is colorless and odorless', 'metadata':'ACETONE'})
print(out)

# COMMAND ----------

# MAGIC %md
# MAGIC Create a LLM chain with our external model. Feed in the new context and question to it.

# COMMAND ----------

from langchain.chains import LLMChain
promptTemplate = PromptTemplate(
         template=configs['prompt_template'], input_variables=["context", "question"])
qa_chain = LLMChain(llm=llmai, prompt=promptTemplate, verbose=True)


# COMMAND ----------

# MAGIC %md
# MAGIC Create the sequential Chain that ties the three chains together.

# COMMAND ----------

from langchain.globals import set_verbose
from langchain.callbacks import StdOutCallbackHandler
set_verbose(True)
handler = StdOutCallbackHandler()
overall_chain = SequentialChain(
                input_variables=['input', 'metadata'],
                #memory=SimpleMemory(memories={"budget": "100 GBP"}),
                chains=[agent, retrieval_chain, qa_chain],
                callbacks=[handler],
                verbose=True)
overall_chain({'input':'what are the health risks of acetone? give all the information you can', 'metadata':'ACETONE'})

# COMMAND ----------

# MAGIC %md
# MAGIC Test with another input

# COMMAND ----------

overall_chain({'input':'what should we do if OSHA is involved', 'metadata':'ACETONE'})

# COMMAND ----------

overall_chain({'input':'what issues can acetone cause with prolonged exposure', 'metadata':'ACETONE'})

# COMMAND ----------


